{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read stats csv\n",
    "\n",
    "df = pd.read_csv('nba_test_stats')\n",
    "advance_df = pd.read_csv('nba_advance_stats')\n",
    "full_df = pd.read_csv('full_stats')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature lists\n",
    "\n",
    "test_features = df[[\"teamAST\", \"teamTO\", \"teamSTL\", \"teamBLK\", \"teamPF\", \"teamFGA\",\n",
    "                        \"team2PM\", \"team3PM\",\"opptAST\", \"opptTO\", \"opptSTL\", \"opptBLK\", \"opptPF\", \"opptFGA\", \n",
    "                        \"oppt2PM\", \"oppt3PM\"]]\n",
    "\n",
    "full_features = full_df[[\"teamAST\", \"teamTO\", \"teamSTL\", \"teamBLK\", \"teamPF\", \"teamFGA\",\n",
    "                        \"team2PM\", \"team3PM\",\"opptAST\", \"opptTO\", \"opptSTL\", \"opptBLK\", \"opptPF\", \"opptFGA\", \n",
    "                              'teamASST%', 'teamEFG%', 'teamAST/TO', 'teamTO%']]\n",
    "\n",
    "top_7 = full_df[['teamEFG%','opptAST', 'teamAST/TO','teamAST', 'teamPF', 'opptPF', 'teamTO%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encode binary classes for win/loss category\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "X = full_features\n",
    "y = full_df['teamRslt']\n",
    "\n",
    "label_encoder.fit(y)\n",
    "y_encoded = label_encoder.transform(y)\n",
    "y_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "\n",
    "# one_hot_y = to_categorical(y_encoded)\n",
    "# one_hot_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test and train data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7035230352303523"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create tree model and test accuracy\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7937669376693767"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest model, fit data, and score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test column values\n",
    "rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05574478, 0.02534397, 0.03656996, 0.04169606, 0.05460816,\n",
       "       0.03965028, 0.04599441, 0.03914606, 0.12502504, 0.04277007,\n",
       "       0.033098  , 0.03126646, 0.05377598, 0.04027698, 0.04200819,\n",
       "       0.18445066, 0.06207673, 0.0464982 ])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rate feature importances\n",
    "importances = rf.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.18445066042327027, 'teamEFG%'),\n",
       " (0.12502503668920262, 'opptAST'),\n",
       " (0.062076733212306376, 'teamAST/TO'),\n",
       " (0.05574478038925502, 'teamAST'),\n",
       " (0.054608161611231935, 'teamPF'),\n",
       " (0.05377598098129356, 'opptPF'),\n",
       " (0.04649820485828048, 'teamTO%'),\n",
       " (0.04599441211585407, 'team2PM'),\n",
       " (0.042770072748136496, 'opptTO'),\n",
       " (0.04200818666385201, 'teamASST%'),\n",
       " (0.04169605735608953, 'teamBLK'),\n",
       " (0.04027698372291808, 'opptFGA'),\n",
       " (0.03965028392037485, 'teamFGA'),\n",
       " (0.03914605654123691, 'team3PM'),\n",
       " (0.03656996018631964, 'teamSTL'),\n",
       " (0.03309799690562321, 'opptSTL'),\n",
       " (0.03126646189596069, 'opptBLK'),\n",
       " (0.025343969778794242, 'teamTO')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(rf.feature_importances_, X), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.815504156125768\n",
      "Testing Data Score: 0.8132791327913279\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3662</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3676</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3677</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3688</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3689</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3690 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prediction  Actual\n",
       "0              0       0\n",
       "1              1       1\n",
       "2              0       0\n",
       "3              1       1\n",
       "4              0       1\n",
       "5              0       0\n",
       "6              1       1\n",
       "7              0       0\n",
       "8              1       0\n",
       "9              1       1\n",
       "10             0       0\n",
       "11             1       1\n",
       "12             0       0\n",
       "13             0       0\n",
       "14             0       0\n",
       "15             1       0\n",
       "16             1       1\n",
       "17             0       0\n",
       "18             0       1\n",
       "19             0       1\n",
       "20             0       0\n",
       "21             1       1\n",
       "22             1       1\n",
       "23             0       1\n",
       "24             1       1\n",
       "25             1       1\n",
       "26             0       0\n",
       "27             0       0\n",
       "28             0       0\n",
       "29             0       0\n",
       "...          ...     ...\n",
       "3660           0       0\n",
       "3661           0       0\n",
       "3662           1       1\n",
       "3663           1       1\n",
       "3664           1       0\n",
       "3665           1       1\n",
       "3666           1       1\n",
       "3667           1       1\n",
       "3668           1       1\n",
       "3669           0       0\n",
       "3670           1       1\n",
       "3671           1       1\n",
       "3672           0       0\n",
       "3673           1       1\n",
       "3674           1       1\n",
       "3675           1       1\n",
       "3676           0       0\n",
       "3677           0       0\n",
       "3678           0       0\n",
       "3679           1       1\n",
       "3680           1       1\n",
       "3681           1       1\n",
       "3682           0       0\n",
       "3683           0       0\n",
       "3684           1       1\n",
       "3685           0       0\n",
       "3686           1       1\n",
       "3687           1       1\n",
       "3688           0       0\n",
       "3689           1       0\n",
       "\n",
       "[3690 rows x 2 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a normal neural network with 2 inputs, 6 hidden nodes, and 2 outputs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=6, activation='relu', input_dim=18))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 6)                 114       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 128\n",
      "Trainable params: 128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 90s - loss: 0.5075 - acc: 0.7512\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.4402 - acc: 0.7913\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.4227 - acc: 0.8008\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.4148 - acc: 0.8064\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.4098 - acc: 0.8064\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.4062 - acc: 0.8076\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.4037 - acc: 0.8101\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.4020 - acc: 0.8108\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.4004 - acc: 0.8104\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.3998 - acc: 0.8121\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.3989 - acc: 0.8107\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.3986 - acc: 0.8126\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.3981 - acc: 0.8119\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.3978 - acc: 0.8112\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.3976 - acc: 0.8122\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.3973 - acc: 0.8135\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.3970 - acc: 0.8130\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.3966 - acc: 0.8134\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.3964 - acc: 0.8116\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.3961 - acc: 0.8146\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.3961 - acc: 0.8132\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.3959 - acc: 0.8137\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.3958 - acc: 0.8149\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.3955 - acc: 0.8128\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.3954 - acc: 0.8139\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.3951 - acc: 0.8137\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.3949 - acc: 0.8123\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.3948 - acc: 0.8139\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.3943 - acc: 0.8138\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.3946 - acc: 0.8152\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.3941 - acc: 0.8142\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.3940 - acc: 0.8139\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.3940 - acc: 0.8137\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.3938 - acc: 0.8132\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.3933 - acc: 0.8147\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.3934 - acc: 0.8139\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.3935 - acc: 0.8147\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.3932 - acc: 0.8139\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.3932 - acc: 0.8157\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.3929 - acc: 0.8142\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.3931 - acc: 0.8154\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.3928 - acc: 0.8142\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.3928 - acc: 0.8140\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.3926 - acc: 0.8148\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.3924 - acc: 0.8151\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.3924 - acc: 0.8162\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.3923 - acc: 0.8148\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.3920 - acc: 0.8161\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.3922 - acc: 0.8144\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.3921 - acc: 0.8141\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.3921 - acc: 0.8143\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.3919 - acc: 0.8138\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.3917 - acc: 0.8156\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.3916 - acc: 0.8141\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.3916 - acc: 0.8169\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.3915 - acc: 0.8154\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.3916 - acc: 0.8163\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.3912 - acc: 0.8148\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.3912 - acc: 0.8160\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.3913 - acc: 0.8155\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.3914 - acc: 0.8148\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.3911 - acc: 0.8162\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.3909 - acc: 0.8165\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.3911 - acc: 0.8160\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.3909 - acc: 0.8147\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.3910 - acc: 0.8150\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.3908 - acc: 0.8153\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.3908 - acc: 0.8151\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.3905 - acc: 0.8175\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.3905 - acc: 0.8153\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.3908 - acc: 0.8162\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.3902 - acc: 0.8160\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.3904 - acc: 0.8154\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.3902 - acc: 0.8163\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.3902 - acc: 0.8144\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.3904 - acc: 0.8169\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.3903 - acc: 0.8176\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.3902 - acc: 0.8131\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.3900 - acc: 0.8148\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.3901 - acc: 0.8170\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.3899 - acc: 0.8161\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.3898 - acc: 0.8155\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.3898 - acc: 0.8160\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.3897 - acc: 0.8174\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.3899 - acc: 0.8152\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.3896 - acc: 0.8143\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.3896 - acc: 0.8162\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.3896 - acc: 0.8168\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.3899 - acc: 0.8141\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.3893 - acc: 0.8168\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.3894 - acc: 0.8169\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.3893 - acc: 0.8175\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.3894 - acc: 0.8151\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.3892 - acc: 0.8161\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.3891 - acc: 0.8160\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.3892 - acc: 0.8167\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.3893 - acc: 0.8159\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.3891 - acc: 0.8169\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.3891 - acc: 0.8165\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.3890 - acc: 0.8167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e300695dd8>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model = Sequential()\n",
    "deep_model.add(Dense(units=6, activation='relu', input_dim=18))\n",
    "deep_model.add(Dense(units=6, activation='relu'))\n",
    "deep_model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 6)                 114       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 170\n",
      "Trainable params: 170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 42s - loss: 0.5693 - acc: 0.7157\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.4261 - acc: 0.8004\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.4118 - acc: 0.8085\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.4077 - acc: 0.8108\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.4048 - acc: 0.8125\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.4030 - acc: 0.8123\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.4015 - acc: 0.8118\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.4004 - acc: 0.8128\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.3995 - acc: 0.8148\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.3990 - acc: 0.8139\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.3984 - acc: 0.8140\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.3981 - acc: 0.8149\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.3976 - acc: 0.8146\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.3973 - acc: 0.8152\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.3969 - acc: 0.8144\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.3961 - acc: 0.8152\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.3962 - acc: 0.8155\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.3960 - acc: 0.8132\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.3953 - acc: 0.8151\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.3954 - acc: 0.8152\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.3952 - acc: 0.8150\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.3949 - acc: 0.8160\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.3942 - acc: 0.8148\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.3941 - acc: 0.8167\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.3939 - acc: 0.8151\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.3941 - acc: 0.8150\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.3939 - acc: 0.8155\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.3936 - acc: 0.8156\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.3936 - acc: 0.8151\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.3930 - acc: 0.8163\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.3935 - acc: 0.8156\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.3928 - acc: 0.8176\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.3929 - acc: 0.8159\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.3928 - acc: 0.8156\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.3924 - acc: 0.8167\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.3924 - acc: 0.8175\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.3922 - acc: 0.8175\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.3919 - acc: 0.8168\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.3915 - acc: 0.8174\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.3917 - acc: 0.8174\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.3914 - acc: 0.8177\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.3911 - acc: 0.8175\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.3909 - acc: 0.8180\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.3910 - acc: 0.8176\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.3908 - acc: 0.8167\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.3905 - acc: 0.8171\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.3905 - acc: 0.8181\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.3900 - acc: 0.8184\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.3902 - acc: 0.8177\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.3901 - acc: 0.8192\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.3899 - acc: 0.8171\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.3901 - acc: 0.8173\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.3900 - acc: 0.8156\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.3897 - acc: 0.8179\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.3896 - acc: 0.8158\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.3895 - acc: 0.8161\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.3896 - acc: 0.8178\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.3894 - acc: 0.8185\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.3894 - acc: 0.8180\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.3892 - acc: 0.8173\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.3891 - acc: 0.8198\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.3887 - acc: 0.8189\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.3889 - acc: 0.8184\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.3890 - acc: 0.8186\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.3889 - acc: 0.8182\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.3888 - acc: 0.8188\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.3890 - acc: 0.8187\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.3887 - acc: 0.8195\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.3883 - acc: 0.8186\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.3887 - acc: 0.8186\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.3885 - acc: 0.8193\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.3881 - acc: 0.8186\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.3880 - acc: 0.8189\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.3882 - acc: 0.8195\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.3881 - acc: 0.8190\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.3880 - acc: 0.8199\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.3881 - acc: 0.8178\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.3878 - acc: 0.8187\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.3880 - acc: 0.8178\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.3879 - acc: 0.8178\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.3877 - acc: 0.8194\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.3878 - acc: 0.8184\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.3876 - acc: 0.8180\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.3874 - acc: 0.8199\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.3877 - acc: 0.8181\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.3871 - acc: 0.8195\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.3872 - acc: 0.8201\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.3875 - acc: 0.8180\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.3872 - acc: 0.8171\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.3871 - acc: 0.8184\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.3869 - acc: 0.8177\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.3870 - acc: 0.8181\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.3871 - acc: 0.8191\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.3869 - acc: 0.8187\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.3866 - acc: 0.8198\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.3868 - acc: 0.8182\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.3869 - acc: 0.8188\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.3866 - acc: 0.8168\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.3863 - acc: 0.8176\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.3864 - acc: 0.8179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e3025754e0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.compile(optimizer='adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "deep_model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python36)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
